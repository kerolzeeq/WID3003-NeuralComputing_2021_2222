{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10. Text Classification  With Simple RNNs _2021_22.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMApCTjpBYBwmzyBT8MBfpF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Text Classification Example with Simple Recurrent Neural Network (RNN)\n",
        "\n",
        "Source: https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks\n",
        "\n",
        "Some examples uses PyTorch Field, which is deprecated. So you need to look for the latest examples when Googling about. Remember that for text classification or NLP tasks, the text not only refers to the characters/words/sentences in the language but includes others such as punctuation mark, commas, exclamation mark etc. For text classification/nlp tasks, you'll need to install dependecies such as torchtext, spacy etc. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bhccwHuvOzJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torchtext\n",
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HtDBaqBtQlm",
        "outputId": "05d5b371-1714-4b71-e30f-52fa4baad327"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.25.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building vocabulary\n",
        "\n",
        "A slightly tricky part for text classification/nlp task is the tokenization-vectorization process where we are converting the words in the text into numbers/vectors. Be aware that tokenization can also be done on character level (eg. character classification/prediction task).\n",
        "\n",
        "In this example functions such as `get_tokenizer` and `build_vocab_from_iterator` are utilised. When dealing with text datasets it is commons for python generators to be used. \n",
        "\n",
        "Please take note that in this example only `<UNK>` is used and there is no `<pad>` . Most probably because the vocabulary takes all the unique words from the dataset. In cases where the vocabulary is high, it is common to take the most frequent words and ignore low frequency words (such as Pneumonoultramicroscopicsilicovolcanoconiosis ) ."
      ],
      "metadata": {
        "id": "PaOtrIUqm50K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGg0egM2sLRn",
        "outputId": "0b76efd3-6582-4ed3-965c-5835144e8de4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98635"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data \n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Iterable\n",
        "\n",
        "\n",
        "train, test = torchtext.datasets.AG_NEWS()\n",
        "\n",
        "labels = [\"world\",\"sports\",\"biz\",\"science\"]\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer  =  get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocab(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocab([train, test]), specials=[\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "len(vocab.get_itos())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the index of the vocabulary is shown. Each unique word has its own index."
      ],
      "metadata": {
        "id": "NEfPKbCSoikj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"Hello how are you?, Welcome to CoderzColumn!!\")\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "print(\"Indexes for the sentence 'what is your name?'\")\n",
        "print(vocab([\"what\",\"is\",\"your\",\"name\"]))\n",
        "\n",
        "print(\"\\n Tokens and indexes for the sentence 'Hello how are you?, Welcome to CoderzColumn!!'\")\n",
        "tokens, indexes\n"
      ],
      "metadata": {
        "id": "iT_fLRo_scTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269ac2a8-041e-498e-c93f-1962e3cb11a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes for the sentence 'what is your name?'\n",
            "[183, 21, 379, 971]\n",
            "\n",
            " Tokens and indexes for the sentence 'Hello how are you?, Welcome to CoderzColumn!!'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['hello',\n",
              "  'how',\n",
              "  'are',\n",
              "  'you',\n",
              "  '?',\n",
              "  ',',\n",
              "  'welcome',\n",
              "  'to',\n",
              "  'coderzcolumn',\n",
              "  '!',\n",
              "  '!'],\n",
              " [12388, 355, 42, 164, 80, 3, 3298, 4, 0, 747, 747])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_dataset, test_dataset  = torchtext.datasets.AG_NEWS()\n",
        "train_dataset, test_dataset  = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "max_words = 25\n",
        "\n",
        "def vectorize_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y) - 1 ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)"
      ],
      "metadata": {
        "id": "V5p98Ex-sfM8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the RNN\n",
        "\n",
        "In this example one embedding layer followed by RNN and then followed by a linear layer. "
      ],
      "metadata": {
        "id": "Rg7ygs9Novqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "embed_len = 50\n",
        "hidden_dim = 50\n",
        "n_layers=1\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.rnn = nn.RNN(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim))\n",
        "        return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "yYzjnnmKsiKE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_classifier = RNNClassifier()\n",
        "\n",
        "rnn_classifier"
      ],
      "metadata": {
        "id": "3vw3X2i0sm68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b70d03-936e-4e13-fe60-c9e4d2701094"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNClassifier(\n",
              "  (embedding_layer): Embedding(98635, 50)\n",
              "  (rnn): RNN(50, 50, batch_first=True)\n",
              "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in rnn_classifier.children():\n",
        "    print(\"Layer : {}\".format(layer))\n",
        "    print(\"Parameters : \")\n",
        "    for param in layer.parameters():\n",
        "        print(param.shape)\n",
        "    print()"
      ],
      "metadata": {
        "id": "11ZD26AsspBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99bbe71-463f-4fd3-c035-4689e4b1f28e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer : Embedding(98635, 50)\n",
            "Parameters : \n",
            "torch.Size([98635, 50])\n",
            "\n",
            "Layer : RNN(50, 50, batch_first=True)\n",
            "Parameters : \n",
            "torch.Size([50, 50])\n",
            "torch.Size([50, 50])\n",
            "torch.Size([50])\n",
            "torch.Size([50])\n",
            "\n",
            "Layer : Linear(in_features=50, out_features=4, bias=True)\n",
            "Parameters : \n",
            "torch.Size([4, 50])\n",
            "torch.Size([4])\n",
            "\n"
          ]
        }
      ]
    }
  ]
}